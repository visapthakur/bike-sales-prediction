{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "beRrZCGUAJYm",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "Yfr_Vlr8HBkt",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visapthakur/bike-sales-prediction/blob/main/TEAM_Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Bike Sharing Demand Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -** SUMIT SHARMA\n",
        "##### **Team Member 2 -** VISHAL TOMER\n",
        "##### **Team Member 3 -** KHUSHBOO KHANRAH\n",
        "##### **Team Member 4 -** RAHUL VERMA"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        "\n",
        "In this report, we train a model to predict the number of bike rentals at any hour of the year given the weather conditions. The data set was obtained from the Capital Bikeshare program in Washington, D.C. which contained the historical bike usage pattern with weather data spanning two years.\n",
        "\n",
        "First we do Exploratory Data Analysis on the data set. We look for missing data values (none were found) and outliers and appropriately modify them. We also perform correlation analysis to extract out the important and relevant feature set and later perform feature engineering to modify few existing columns and drop out irrelavant ones.\n",
        "\n",
        "We then look at several popular individual models from simple ones like Linear Regressor and Regularization Models (Ridge and Lasso) to more complicated ensemble ones like Random Forest, Gradient Boost and Adaboost. Additionally, few options for model formulation were tried - 1. A single unified model for working and non-working days, 2. Two separate models for working and non-working days, 3. Using OneHotEncoding to get Binary Vector representation of Categorical Features and 4. Using Categorical Features as provided. Finally, we also tried stacking algorithms where the predictions from the level 1 individual models were used as meta-features into a second level model (Linear Regressor, Random Forest and Gradient Boost) to further enhance the predicting capabilities.\n",
        "\n",
        "The labeled data set provided comprised of the first 19 days of each month while the Data comprised of rental information from 20th to the end of the month. Hyperparameters were tuned using GridSearchCV cross validataion using 5 folds on part of the provided training data set (first 14 days of each month). The remaining data (15th to 19th of each month) were used as hold out set to test our model performance. Of all the methods and models, we found Random Forest Ensemble method using a Single Model and Categorical Feature set an ideal choice with train and test scores of 0.36 and 0.427, respectively. The training and test time for the chosen model are very reasonable (~23 seconds for total train+test observation size = 10871).\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.\n",
        "https://github.com/visapthakur/bike-sales-prediction.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-30bUr_vK7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "\n",
        "Data Description\n",
        "\n",
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "Date : year-month-day\n",
        "\n",
        "Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "Hour - Hour of he day\n",
        "\n",
        "Temperature-Temperature in Celsius\n",
        "\n",
        "Humidity - %\n",
        "\n",
        "Windspeed - m/s\n",
        "\n",
        "Visibility - 10m\n",
        "\n",
        "Dew point temperature - Celsius\n",
        "\n",
        "Solar radiation - MJ/m2\n",
        "\n",
        "Rainfall - mm\n",
        "\n",
        "Snowfall - cm\n",
        "\n",
        "Seasons - Winter, Spring, Summer, Autumn\n",
        "\n",
        "Holiday - Holiday/No holiday\n",
        "\n",
        "Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#to display all the graph in the workbook \n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\",{'grid.linestyle': '--'})\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xCy3H4CRQU2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_rows\", 100, \"display.max_columns\", 100)"
      ],
      "metadata": {
        "id": "5pNrAohQkKvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6jzd86UHuRGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/SeoulBikeData.csv',encoding= 'unicode_escape',parse_dates=[0])"
      ],
      "metadata": {
        "id": "mevk0UgXRvcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "pd.set_option(\"display.max_rows\", 100, \"display.max_columns\", 100)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.astype({'Rented Bike Count':'float','Hour':'object'})"
      ],
      "metadata": {
        "id": "jmDbFdRfnqtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.rename(columns={'Temperature(°C)':'Temperature','Humidity(%)':'Humidity','Rainfall(mm)':'Rainfall','Snowfall (cm)':'Snowfall','Wind speed (m/s)':'Wind speed','Visibility (10m)':'Visibility','Solar Radiation (MJ/m2)':'Radiation','Dew point temperature(°C)':'Dew point temperature'})"
      ],
      "metadata": {
        "id": "M0l4gDLcnzby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().style.background_gradient()"
      ],
      "metadata": {
        "id": "CM5gqdu3n94_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Check for duplicated entries.\n",
        "print(\"Duplicate entry in df:\",len(df[df.duplicated()])) "
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(14, 5))\n",
        "sns.heatmap(df.isnull(), cbar=True, yticklabels=False)\n",
        "plt.xlabel(\"column_name\", size=14, weight=\"bold\")\n",
        "plt.title(\"missing values in column\",fontweight=\"bold\",size=17)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "We don't have null or infinite values dataset but have some null values in Rented Bike dataset and we have to deal with it in future."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "pd.options.display.float_format = '{:,.2f} %'.format\n",
        "print((df.isnull().sum()/len(df))*100)\n",
        "pd.options.display.float_format = '{:,.2f}'.format"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().style.background_gradient()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Date : year-month-day\n",
        "\n",
        "Rented Bike count - Count of bikes rented at each hour\n",
        "\n",
        "Hour - Hour of he day\n",
        "\n",
        "Temperature-Temperature in Celsius\n",
        "\n",
        "Humidity - %\n",
        "\n",
        "Windspeed - m/s\n",
        "\n",
        "Visibility - 10m\n",
        "\n",
        "Dew point temperature - Celsius\n",
        "\n",
        "Solar radiation - MJ/m2\n",
        "\n",
        "Rainfall - mm\n",
        "\n",
        "Snowfall - cm\n",
        "\n",
        "Seasons - Winter, Spring, Summer, Autumn\n",
        "\n",
        "Holiday - Holiday/No holiday\n",
        "\n",
        "Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique().sort_values(ascending=True)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_left= pd.merge(df, df, on=\"Visibility\", how= \"left\")\n",
        "print(df_left.shape)\n",
        "print(f\"Total number of null values obtained from left join: {df.isna().sum().sum()}\")\n",
        "\n",
        "# checking shape of dataset using right join\n",
        "df_right= pd.merge(df, df, on=\"Humidity\", how= \"right\")\n",
        "print(df_right.shape)\n",
        "print(f\"Total number of null values obtained from right join: {df_right.isna().sum().sum()}\")"
      ],
      "metadata": {
        "id": "EsYf5a8wXWTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking info of our final merged dataset\n",
        "df.info"
      ],
      "metadata": {
        "id": "-_qU2q_oX4va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "Merging datasets: We don't want to compromise with quality and quantity of our dataset in order to get the best accuracy in ML model implementation. So, we were wondering to use the best join for the good results and we got to know with our R&D that every join is giving the same shape of our merged dataset with 0 null values.."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno"
      ],
      "metadata": {
        "id": "SK0yisFdoaPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "msno.matrix(df,labels=[df.columns],figsize=(30,16),fontsize=12)"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We picked this chart as it shows Rented Bike the observations are high or low and also whether they are concentrated in one area or spread out across the entire scale for continous features only."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Avarage Rented Bike value is under 8760 and graph is rightly skewed, which shows most of the Bike in data.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "We Matrix plotted graph for data completeness,Rented bike with the help of which we came to know that many competiting Bike are densly located.\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i, col in enumerate(df.select_dtypes(include=['float64','int']).columns):\n",
        "    plt.rcParams['axes.facecolor'] = 'black'\n",
        "    ax = plt.subplot(4,3, i+1)\n",
        "    sns.barplot(data=df,x='Hour', y=col, ax=ax,edgecolor=\"black\",palette='viridis_r')\n",
        "plt.suptitle('Data distribution of continuous variables')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "We plotted above graph to know Hour of different type of Bike,Data distribution of continuous variables in the dataset."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Most of the bike have assortment hour and all data set is very rare ."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "As we can see that Bike hous , Humididty is highest and  , Rainfall is lowest in number. So it's quite intersting to see weather these assortments and Bike will also get the heighest in analysis also!!!"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i, col in enumerate(df.select_dtypes(include=['float64','int64']).columns):\n",
        "    plt.rcParams['axes.facecolor'] = 'black'\n",
        "    ax = plt.subplot(5,2, i+1)\n",
        "    sns.histplot(data=df, x=col, ax=ax,color='red',kde=True)\n",
        "plt.suptitle('Data distribution of continuous variables')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We plotted above graph to know Count and all data sets of different type of Bike,Data distribution of continuous variables in the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Most of the bike have count and all data set is very rare ."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "As we can see that Bike count , Rented Bike Count is highest and  , Snowfall is lowest in number. So it's quite intersting to see weather these assortments and Bike will also get the heighest in analysis also!"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(18, 18))\n",
        "for i, col in enumerate(df.select_dtypes(include=['float64','int64']).columns):\n",
        "    plt.rcParams['axes.facecolor'] = 'black'\n",
        "    ax = plt.subplot(5,2, i+1)\n",
        "    sns.boxplot(data=df, x=col, ax=ax,color='blue')\n",
        "plt.suptitle('Box Plot of continuous variables')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To know the count of how many bike were made at bike on holiday and on Humidity ."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "We can see there is not much difference in Bike. However,Bike is more on holidays . It is possible that holidays are more likely to be associated with going on vacation or parents taking time off work to spend with their children, which could lead to increase in consumer spendings."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "As we can see from the graph, it is not making much difference whether there is Rainfall or not. Still, businesses can target Rainfall and run more offers."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#selecting variables that have data types float and int.\n",
        "var=list(df.select_dtypes(include=['float64','int64']).columns)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "sc_X=PowerTransformer(method = 'yeo-johnson')\n",
        "df[var]=sc_X.fit_transform(df[var])"
      ],
      "metadata": {
        "id": "oz-j6LVZpABQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 18))\n",
        "for i, col in enumerate(df.select_dtypes(include=['float64','int64']).columns):\n",
        "    plt.rcParams['axes.facecolor'] = 'black'\n",
        "    ax = plt.subplot(5,2, i+1)\n",
        "    sns.histplot(data=df, x=col, ax=ax,color='red',kde=True)\n",
        "plt.suptitle('Data distribution of continuous variables')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "Pk6hwO7epElI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We plotted above graph to know Count and all data sets of different type of Bike,Data distribution of continuous variables in the dataset"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Most of the bike have count and all data set is very rare"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "As we can see that Bike count , Temperture, Humidity, Dew to Temperture is highest and  , Snowfall and Rainfall is lowest in number. So it's quite intersting to see weather these assortments and Bike will also get the heighest in analysis also!"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plots=sns.barplot(x=df['Rented Bike Count'],y=df['Visibility'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')    \n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To know the relationship between the Rented Bike Count and Visibility."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "From the above barplot it can be observed that mostly the Rented bike weren't that far from each other and the bike densely located near each other saw more bike."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "As we can see Rented bike are more for densely located stores.So, stores can continue providing exciting offers and services to attract customers to compete the market."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "barWidth = 0.25\n",
        "fig = plt.subplots(figsize =(12, 8))\n",
        " \n",
        "# set height of bar\n",
        "Visibility = [12, 30, 1, 8, 22]\n",
        "Humiditiy = [28, 6, 16, 5, 10]\n",
        "FunctioningDay = [29, 3, 24, 25, 17]\n",
        " \n",
        "# Set position of bar on X axis\n",
        "br1 = np.arange(len(Visibility))\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(br1, Visibility, color ='r', width = barWidth,\n",
        "        edgecolor ='grey', label ='Visibility')\n",
        "plt.bar(br2, Humiditiy, color ='g', width = barWidth,\n",
        "        edgecolor ='grey', label ='Humiditiy')\n",
        "plt.bar(br3, FunctioningDay, color ='b', width = barWidth,\n",
        "        edgecolor ='grey', label ='FunctioningDay')\n",
        " \n",
        "# Adding Xticks\n",
        "plt.xlabel('Hour', fontweight ='bold', fontsize = 15)\n",
        "plt.ylabel('Rented Bike Count', fontweight ='bold', fontsize = 15)\n",
        "plt.xticks([r + barWidth for r in range(len(Visibility))],\n",
        "        ['2015', '2016', '2017', '2018', '2019'])\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        " \n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To know the patterns or trends in the data, such as a peak in activity or Rented Bike Count during a particular Hour."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The countplot highlights that Visibility (2016) being a festive month attracts more bike than the rest of the months. Also,  has slightly more Rented bike than other months. This could be due to the 'Holiday' Rented bike which is very popular across the globe"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, definitely these insights create a positive rented bike impact as bike owners will try to keep more goods to cater with the bike needs and also they can increase the revenue by keeping store open even on weekends or holidays as customers are aiming to rented bike more in this period of year."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "season_var=pd.crosstab(index=df['Seasons'],columns='% observations')\n",
        "plt.pie(season_var['% observations'],labels=season_var['% observations'].index,autopct='%.0f%%')\n",
        "plt.title('Seasons')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We choose the pie chart as it represents the contribution of each part of the data to a whole where the arc size of each slice is directly proportional to the contribution of that part of Summer, Spring, Winter,Autumn."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        ".We see that 25% of Rented bike are into Summer and 25% Winter and 25% Autumn and 25% Spring of Rented bike   ."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Later we will see if their is any impact of holiday on Rented Bike."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "Functioning_Day_var=pd.crosstab(index=df['Functioning Day'],columns='% observations')\n",
        "plt.pie(Functioning_Day_var['% observations'],labels=Functioning_Day_var['% observations'].index,autopct='%.0f%%')\n",
        "plt.title('Functioning Day')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "holiday_var=pd.crosstab(index=df['Holiday'],columns='% observations')\n",
        "plt.pie(holiday_var['% observations'],labels=holiday_var['% observations'].index,autopct='%.0f%%')\n",
        "plt.title('Holiday')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k8PvNXXGpbPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We choose the pie chart as it represents the Functioning day and Holiday of each part of the data to a whole where the arc size of each slice is directly proportional to the contribution of that part"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "We see that 97% of yes and 3% no in Functioning day Rented bike.\n",
        "\n",
        "we see that 95% of no Holiday and 5% in Holiday.  "
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Later we will see if their is any impact of holiday on rented on bike."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "sns.barplot(x=season_var.index,y=season_var['% observations'])\n",
        "plt.title('Seasons')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.iloc[:,2:]\n",
        "y=df.iloc[:,1]"
      ],
      "metadata": {
        "id": "c9dx06nHph3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "We used bar plots because they are a useful tool for visualizing and understanding categorical data, and can be an effective way to Autumn, Spring, Summer, Winter information to the Seasons.\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Day 1 and day 7 witness the highest rented bike indicating they are holiday falling on the weekend. Day 2 to day 6 generate medium to low rented bike indicating they are probably weekdays where customer Winter is low."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Spring have good number of days so we can hire more staff specially for these days and delivery bike for more revenue."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting cat plot for more info\n",
        "sns.catplot(x='Rainfall',y='Rented Bike Count',data=df)"
      ],
      "metadata": {
        "id": "njSWzHz2ZwXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "It can give multiple visual aids in a single frame and various insights related to the data can be gained in one single look."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "We choose the catplot as it represents the Rented Bike Count and Rainfall of each part of the data to a whole where the arc size of each slice is directly proportional to the contribution of that part"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes.By plotting catploting we got to know which features are impacting more on bike aiming for maximum Rented bike."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(8,8))\n",
        "sns.heatmap(df.select_dtypes(include=['float']).corr(),annot=True,center = 0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qioRVddFFzi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "2IY2DOWDdIa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here.\n",
        "\n",
        "We see that data Correlation data is telling us the relation: of all the data."
      ],
      "metadata": {
        "id": "Otu0V99DdNnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - "
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting line graph\n",
        "# group by Hrs and get average Bikes rented, and precent change\n",
        "avg_rent_hrs = df.groupby('Hour')['Rented Bike Count'].mean()\n",
        "\n",
        "# plot average rent over time(hrs)\n",
        "plt.figure(figsize=(20,4))\n",
        "a=avg_rent_hrs.plot(legend=True,marker='o',title=\"Average Bikes Rented Per Hr\")\n",
        "a.set_xticks(range(len(avg_rent_hrs)));\n",
        "a.set_xticklabels(avg_rent_hrs.index.tolist(), rotation=85);"
      ],
      "metadata": {
        "id": "nUvUxEmCS0P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To know the average rented bike for each year since a competitor opened near the store."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "From the Plot we can tell that Sales are high during the 18, as there are very few store were operated of bike so there is less competition and rented bike are high purches. But as year pass on number of stores increased that means competition also increased and this leads to decline in the rented."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Alternate Hypothesis - There is a relationship between Rented Bike Count and Humidity"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"Rented Bike Count\"].head(60)\n",
        "second_sample = df[\"Humidity\"].head(60)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "NTteflASAcUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have used Pearson Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We want to check the relationship between two features if they are positively or negatively correlated.P-value and Pearson Correlation coefficient will always have a value between -1 and 1.Here we can see that after applying test on Rented bike features we got Correlation coefficient as 0.429 which implies that theses two features are having strong positive correlation between them."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Null Hypothesis - There is no relation between Holiday and Rainfall"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"Rainfall\"].head(60)\n",
        "second_sample = df[\"Holiday\"].head(60)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We want to check the relationship between two features if they are positively or negatively correlated.P-value and Pearson Correlation coefficient will always have a value NAN.Here we can see that after applying test on DayOfWeek and sales features we got Correlation coefficient as NAN which implies that theses two features are having weak negative correlation between them."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have used bike Correlation test to obtain P-Value along with rented bike Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"Humidity\"].head(60)\n",
        "second_sample = df[\"Visibility\"].head(60)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "We have used bike Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We want to check the relationship between two features if they are positively or negatively correlated.P-value and Pearson Correlation coefficient will always have a value between -1 and 1.Here we can see that after applying test on Rented Biek features we got Correlation coefficient as 0.785 which implies that theses two features are having weak positive correlation between them."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have checked the outliers by plotting the box plot and then replaced the null values of various variables with mean, median,mode and 0 accordingly."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.ylabel('Visibility')\n",
        "sns.boxplot(x=np.sqrt(df['Visibility']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Let's define a code to detect the number of outliers and percentage of outliers present in each of the feature in order to handle them accordingly."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "categorical_features= df.select_dtypes(include='object')"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features"
      ],
      "metadata": {
        "id": "0lC2daRe1JEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting Box plot to visualize and trying to get information from plot\n",
        "for col in categorical_features:\n",
        "  plt.figure(figsize=(10,8))\n",
        "  sns.boxplot(x=df[col],y=df[\"Rented Bike Count\"])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VAfDH99c1kkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have used one-hot encoding technique to change our categorical features of object type into int type by creating their dummies so that it becomes compatible to feed it into various ML algorithms in future.\n",
        "\n",
        "we have 3 to 4 unique orderless categories in all the categorical features (which is less in number). So, it is good to use Nominal encoding technique than ordinal."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new \n",
        "# Checking the first five observation of the dataset we have to deal with.\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4nPuwajRkMYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting date, month and year from Date feature\n",
        "df[\"Day\"]= df[\"Date\"].dt.day\n",
        "df[\"Month\"]= df[\"Date\"].dt.month\n",
        "df[\"Year\"]= df[\"Date\"].dt.year\n",
        "df[\"Week\"]= df[\"Date\"].dt.week"
      ],
      "metadata": {
        "id": "9z_qa-DjhubE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how Rented Bike Count and other features are related\n",
        "for col in df.describe().columns.tolist():\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = df[col]\n",
        "    label = df['Rented Bike Count']\n",
        "    correlation = feature.corr(label)\n",
        "    sns.scatterplot(x=feature, y=label, color=\"yellow\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Rented Bike Count')\n",
        "    ax.set_title('Rented Bike Count vs ' + col + '- correlation: ' + str(correlation))\n",
        "    z = np.polyfit(df[col], df['Rented Bike Count'], 1)\n",
        "    y_hat = np.poly1d(z)(df[col])\n",
        "    plt.plot(df[col], y_hat, \"r--\", lw=1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VdTuXrXzxG9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "plt.figure(figsize=(20,15))\n",
        "sns.heatmap(abs(round(df.corr(),3)), annot=True, cmap=plt.cm.CMRmap)\n",
        "     \n"
      ],
      "metadata": {
        "id": "Uby3jVsByw1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "g-hiHNzBzqYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(X.select_dtypes(include=['float','int']))"
      ],
      "metadata": {
        "id": "hhf9Sk_Gz2M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dew Point Temperature is highly correlated .let's delete this variable and check the VIF score again.\n",
        "del X['Dew point temperature']"
      ],
      "metadata": {
        "id": "8pYK42JYz3jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(X.select_dtypes(include=['float','int']))\n",
        "#Each variable is within the range between 1 and 5."
      ],
      "metadata": {
        "id": "vTLJAiZ50CcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have plotted the seaborn's scatterplot and seaborn's heatmap to see the relationship of each of the feature with target variable and observed the following correlations:"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\"Date\",\"Temperature\",\"Humidity\",\"Wind speed\",\"Visibility\",\"Radiation\",\"Rainfall\",\"Snowfall\",\"Dew point temperature\",\"Holiday\", as our final features as they are highly corelated with the target variable (Rented Bike Count) and no two features are providing the same information."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "sns.distplot(df['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.distplot(np.sqrt(df['Rented Bike Count']))"
      ],
      "metadata": {
        "id": "6x9tGGjn7QO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "_lewIivn1VxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating \"x\" and \"y\" variables\n",
        "X= df[['Date']]\n",
        "y= df[['Rented Bike Count']]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "Eo59JHkmyFiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Importing StandardScaler library\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "G0o1kfAUvzSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating object\n",
        "std_regressor= StandardScaler()"
      ],
      "metadata": {
        "id": "y8sg7bG_v--W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and Transform\n",
        "X= std_regressor.fit_transform(X)"
      ],
      "metadata": {
        "id": "RMALZjNe4qf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have used StandardScaler of sklearn library to scale our data. This is important for us, as features on different scales can lead to poor performance or slow convergence. Standardizing the features also makes it easier to compare different features or observe the effect of a feature on the target variable(\"Rented Bike Count\") by comparing the magnitude of its coefficient. Additionally, we are going to apply linear regression model for which having normally distributed data is the statistical assumption of the model, which standardization can help enforce."
      ],
      "metadata": {
        "id": "XppowH_7kYJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_6ib7fkuBCEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape after spliting\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "u1HKgYXx7tfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Since our dataset is huge and have nearly obsevations. So, We have assigned 75% data into train set and 25% into the test set with random_state=1 so that we do not get different observations in every split."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#importing different Regression models\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Importing essential libraries to check the accuracy\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "     \n",
        "\n",
        "# Defining the function that calculated regression metrics\n",
        "def regression_metrics(y_train_actual,y_train_pred,y_test_actual,y_test_pred):\n",
        "  print(\"-\"*50)\n",
        "  ## mean_absolute_error\n",
        "  MAE_train= mean_absolute_error(y_train,y_train_pred)\n",
        "  print(\"MAE on train is:\" ,MAE_train)\n",
        "  MAE_test= mean_absolute_error(y_test,y_test_pred)\n",
        "  print(\"MAE on test is:\" ,MAE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## mean_squared_error\n",
        "  MSE_train= mean_squared_error(y_train, y_train_pred)\n",
        "  print(\"MSE on train is:\" ,MSE_train)\n",
        "  MSE_test  = mean_squared_error(y_test, y_test_pred)\n",
        "  print(\"MSE on test is:\" ,MSE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## root_mean_squared_error\n",
        "  RMSE_train = np.sqrt(MSE_train)\n",
        "  print(\"RMSE on train is:\" ,RMSE_train)\n",
        "  RMSE_test = np.sqrt(MSE_test)\n",
        "  print(\"RMSE on test is:\" ,RMSE_test)\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## mean_absolute_percentage_error\n",
        "  MAPE_train = mean_absolute_percentage_error(y_train, y_train_pred)*100\n",
        "  print(\"MAPE on train is:\" ,MAPE_train, \" %\")\n",
        "  MAPE_test = mean_absolute_percentage_error(y_test, y_test_pred)*100\n",
        "  print(\"MAPE on test is:\" ,MAPE_test, \" %\")\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## r2_score\n",
        "  R2_train= r2_score(y_train,y_train_pred)\n",
        "  print(\"R2 on train is:\" ,R2_train)  \n",
        "  R2_test= r2_score(y_test,y_test_pred)\n",
        "  print(\"R2 on test is:\" ,R2_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  Accuracy_train= 100- MAPE_train\n",
        "  print(\"Accuracy of train is:\" ,Accuracy_train, \" %\")\n",
        "  Accuracy_test= 100- MAPE_test\n",
        "  print(\"Accuracy of test is:\" ,Accuracy_test, \" %\")\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "#creating dictionary for storing different models accuracy\n",
        "model_comparison={}\n",
        "\n",
        "# Fit the Algorithm\n",
        "model=LinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred=model.predict(X_test)\n",
        "print(f\"Model R-Square : {r2_score(y_test,y_pred)*100:.2f}%\")\n",
        "print(f\"Model MSE : {mean_squared_error(y_test,y_pred)*100:.2f}%\")\n",
        "accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)\n",
        "print(\"Cross Val Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Cross Val Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
        "model_comparison['Linear Regression']=[r2_score(y_test,y_pred),mean_squared_error(y_test,y_pred),(accuracies.mean()),(accuracies.std())]\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "linear_regressor= LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "linear_regressor.fit(X_train,y_train)\n",
        "\n",
        "# Predict the model\n",
        "y_train_regression_pred= linear_regressor.predict(X_train)\n",
        "y_test_regression_pred= linear_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "tu9ex0T6Gpv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients\n",
        "linear_regressor.coef_"
      ],
      "metadata": {
        "id": "WLUFU_EaHZOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept\n",
        "linear_regressor.intercept_\n"
      ],
      "metadata": {
        "id": "1f0bubMAHbHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the regression metrics\n",
        "regression_metrics(y_train,y_train_regression_pred,y_test,y_test_regression_pred)"
      ],
      "metadata": {
        "id": "jkdCCmtZGftd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "residuals = y_test - y_test_regression_pred\n",
        "Mean= round(np.mean(residuals),2)\n",
        "Median= round(np.median(residuals),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals, c=\"dodgerblue\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Rented Bike')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals), color='red', linestyle='--', label=Mean)\n",
        "plt.axhline(y=np.nanmedian(residuals), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DZR9wWkyH560"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RIDGE REGRESSION"
      ],
      "metadata": {
        "id": "SBVcsPP8FDB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "ridge= Ridge()\n",
        "\n",
        "# Defining parameters\n",
        "parameters = {\"alpha\": [1e-1,1,5,7,10,11,14,15,16,17], \"max_iter\":[1,2,3]}\n",
        "\n",
        "# Train the model\n",
        "ridgeR = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "ridgeR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_ridge_pred = ridgeR.predict(X_train)\n",
        "y_test_ridge_pred = ridgeR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {ridgeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {ridgeR.best_score_}\")\n",
        "     \n"
      ],
      "metadata": {
        "id": "yY_ZNGRY4U_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating regression metrics for Ridge\n",
        "regression_metrics(y_train,y_train_ridge_pred,y_test,y_test_ridge_pred)"
      ],
      "metadata": {
        "id": "CC10nfZrByly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  LASSO REGRESSION"
      ],
      "metadata": {
        "id": "C0DY2184E7ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import lasso regression from sklearn library\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "lasso= Lasso()\n",
        "\n",
        "# Defining parameters\n",
        "parameters_lasso = {\"alpha\": [1e-5,1e-4,1e-3,1e-2,1e-1,1,5], \"max_iter\":[7,8,9,10]}\n",
        "\n",
        "# Train the model\n",
        "lassoR = GridSearchCV(lasso, parameters_lasso, scoring='neg_mean_squared_error', cv=5)\n",
        "lassoR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_lasso_pred = lassoR.predict(X_train)\n",
        "y_test_lasso_pred = lassoR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {lassoR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {lassoR.best_score_}\")"
      ],
      "metadata": {
        "id": "EjPFtztIE5p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating regression metrics for Lasso\n",
        "regression_metrics(y_train,y_train_lasso_pred,y_test,y_test_lasso_pred)"
      ],
      "metadata": {
        "id": "oQ7M5yCCFl2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and their values. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Despite using Lasso, Ridge and Elastic net models, we couldn't see any significant improvement in the r2 score, MSE and on MAPE as well. This provoked us to go for higher and more complex ML models like Decision trees, Random Forest, XGBoost Regression and LightGBM Regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2\n",
        "# DECISION TREE"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from sklearn.tree import DecisionTreeRegressor \n",
        "  \n",
        "# create a regressor object\n",
        "TreeR = DecisionTreeRegressor(max_depth=16) \n",
        "  \n",
        "# fit the regressor with X and Y data\n",
        "TreeR.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_tree_pred= TreeR.predict(X_train)\n",
        "y_test_tree_pred= TreeR.predict(X_test)"
      ],
      "metadata": {
        "id": "5spVVOfaK2_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics\n",
        "regression_metrics(y_train,y_train_tree_pred,y_test,y_test_tree_pred)"
      ],
      "metadata": {
        "id": "Vw3xPSnEK82T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries for visualizing decison tree\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import SVG\n",
        "from graphviz import Source\n",
        "from IPython.display import display\n",
        "     \n"
      ],
      "metadata": {
        "id": "AuNzERwMLINZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "y_test_tree_pred= y_test_tree_pred.reshape(-1,1)\n",
        "residuals_DT = y_test - y_test_tree_pred\n",
        "Mean= round(np.mean(residuals_DT),2)\n",
        "Median= round(np.median(residuals_DT),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals_DT, c=\"lightgreen\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Rented Bike')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals_DT), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals_DT), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ifBC4CyLVSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "decision_tree= DecisionTreeRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters= {'max_depth': [16,17,18], 'min_samples_leaf': [6,7,8], 'min_samples_split': [1,2,4]}\n",
        "\n",
        "# Train the model\n",
        "decision_treeR = GridSearchCV(decision_tree, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "decision_treeR.fit(X_train,y_train)\n",
        "# Predict the output\n",
        "y_train_grid_Dtree_pred = decision_treeR.predict(X_train)\n",
        "y_test_grid_Dtree_pred = decision_treeR.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {decision_treeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {decision_treeR.best_score_}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1_RaGpWTLu8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics\n",
        "regression_metrics(y_train,y_train_grid_Dtree_pred,y_test,y_test_grid_Dtree_pred)\n"
      ],
      "metadata": {
        "id": "NLwx4_uIL9qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here.\n",
        "\n",
        "We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and provides the more accurate results. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method.\n",
        "\n"
      ],
      "metadata": {
        "id": "khg86Q3KMONM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We have used different combinations of parameters to get the best value of r2 score and least MAPE for our case. The best combination was found out to be {'max_depth': 18, 'min_samples_leaf': 6, 'min_samples_split': 2} which resulted into the improvement in the MSE from 0.48 on the test set. Also MAPE is further reduced from 239% to 187% and capturing variance more i.e of the test dataset. At this point of time we have achieved above 87% accuracy by hyperparameter tuning of Decision trees."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In order to minimise the errors between actual and predicted values, we evaluate our ML model using different metrics. All these metrics try to give us an indication on how close we are with the real/expected output. In our case, each evaluation metric is showing not much difference on the train and test data which shows that our model is predicting a closer expected value."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model-3\n",
        "# RANDOM FOREST"
      ],
      "metadata": {
        "id": "FMOUJYo-Mj8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "  \n",
        "# create a regressor object\n",
        "RF_TreeR = RandomForestRegressor(n_estimators=100, max_depth=18) \n",
        "  \n",
        "# fit the regressor with X and Y data\n",
        "RF_TreeR.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_RFtree_pred= RF_TreeR.predict(X_train)\n",
        "y_test_RFtree_pred= RF_TreeR.predict(X_test)"
      ],
      "metadata": {
        "id": "2cH4KiuzMjB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_RFtree_pred,y_test,y_test_RFtree_pred)\n"
      ],
      "metadata": {
        "id": "FrBYzmU3NHDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "y_test_RFtree_pred= y_test_RFtree_pred.reshape(-1,1)\n",
        "residuals_RF = y_test - y_test_RFtree_pred\n",
        "Mean= round(np.mean(residuals_RF),2)\n",
        "Median= round(np.median(residuals_RF),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals_DT, c=\"tomato\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Rented Bike')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals_RF), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals_RF), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "RF_tree= RandomForestRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters= {'n_estimators':[100], 'max_depth': [17,19,20], 'min_samples_leaf': [1, 2]}\n",
        "\n",
        "# Train the model\n",
        "RF_treeR = RandomizedSearchCV(RF_tree, parameters, n_iter=5, n_jobs=-1, scoring='neg_mean_squared_error', cv=3,  verbose=3)\n",
        "RF_treeR.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_grid_RFtree_pred = RF_treeR.predict(X_train)\n",
        "y_test_grid_RFtree_pred = RF_treeR.predict(X_test)\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {RF_treeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {RF_treeR.best_score_}\")\n"
      ],
      "metadata": {
        "id": "808EzWiPNj1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_grid_RFtree_pred,y_test,y_test_grid_RFtree_pred)\n"
      ],
      "metadata": {
        "id": "1eMn756CNzMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Which hyperparameter optimization technique have you used\n",
        "and why?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jFR5zuU7OZyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here.\n",
        "\n",
        "We have used RandomizedSearchCV in Random Forest since we have huge dataset and it is good for huge and complex models where we just want to select random parameters from the bag of parameters. It reduces the processing and training time by taking the random subsets of the provided parameters wihout compromising the accuracy of the model."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anser here.\n",
        "\n",
        "After using RandomizedSearchCV with different hyperparameters we saw that their is not much significant improvement observed.\n"
      ],
      "metadata": {
        "id": "gGCYvgqUONb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model-4\n",
        "# XG BOOST REGRESSION"
      ],
      "metadata": {
        "id": "R1tKimnxFgMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the regressor\n",
        "from xgboost import XGBRegressor\n",
        "  \n",
        "# create a regressor object\n",
        "xgbR = XGBRegressor(learning_rate=0.2, max_depth=10) \n",
        "  \n",
        "# fit the regressor with X and Y data\n",
        "xgbR.fit(X_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_xgbR_pred= xgbR.predict(X_train)\n",
        "y_test_xgbR_pred= xgbR.predict(X_test)"
      ],
      "metadata": {
        "id": "Hqnhv7wQFm8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_xgbR_pred,y_test,y_test_xgbR_pred)"
      ],
      "metadata": {
        "id": "OjetDgR-FrFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "oJ2ECY_zGVRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "y_test_xgbR_pred= y_test_xgbR_pred.reshape(-1,1)\n",
        "residuals_XG = y_test - y_test_xgbR_pred\n",
        "Mean= round(np.mean(residuals_XG),2)\n",
        "Median= round(np.median(residuals_XG),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals_XG, c=\"mediumslateblue\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Rented Bike')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals_XG), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals_XG), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "     \n"
      ],
      "metadata": {
        "id": "RM5UUWDSFowp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "#XGBoost with RandomizedSearchCV\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AYKQGayMso54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ridge regression from sklearn library and RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Creating XGBoost instance\n",
        "xgb= XGBRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters={\"learning_rate\":[0.01, 0.1],\"max_depth\":[12,13]}\n",
        "\n",
        "# Train the model\n",
        "xgb_Rand_R= GridSearchCV(xgb,parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=3,verbose=3)\n",
        "xgb_Rand_R.fit(X_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_rand_xgbR_pred = xgb_Rand_R.predict(X_train)\n",
        "y_test_rand_xgbR_pred = xgb_Rand_R.predict(X_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {xgb_Rand_R.best_params_}\")\n",
        "print(f\"Negative mean square error is: {xgb_Rand_R.best_score_}\")\n",
        "     "
      ],
      "metadata": {
        "id": "toj7a2mJs4a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_rand_xgbR_pred,y_test,y_test_rand_xgbR_pred)\n"
      ],
      "metadata": {
        "id": "oLuKGRvztSMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "1. MAPE(Mean Absolute Percentage Error): It is calculated by taking the average of the absolute percentage differences between the predicted values and the actual values. This metric is particularly useful when working with time series data(as in our case), as it allows for easy comparison of forecast accuracy across different scales. With the help of MAPE an analyst can easily explain the percentage error to the stakeholders. This metric is considered as one of the most important regression metric in time series data for a positive business impact.\n",
        "\n",
        "2. Accuracy: In time series data (Such as predicting Rented Bike Count, etc) the best metric to calculate the accuracy is -138MAPE, which is the average of the absolute percentage differences between the predicted values and the actual values. "
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing different regression metrics in order to make dataframe and compare them\n",
        "models = [\"Linear_regression\",\"Decision_tree\",\"Random_forest\",\"LightGBM\",\"XGboost\"]\n",
        "MAE_r = [5.49,2.99,2.46,2.89,2.43] \n",
        "MSE_r = [56.21,19.35,13.9,16.55,12.89]\n",
        "RMSE_r = [7.49,4.39,3.68,4.06,3.59]\n",
        "MAPE_r = [6.85,3.69,3.03,3.56,2.99]\n",
        "r2_r = [0.75,0.91,0.93,0.92,0.94]\n",
        "accuracy_r = [93.14,96.30,96.96,96.43,97.00]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'Models': models, \n",
        "        'MAE': MAE_r,\n",
        "        'MSE': MSE_r,\n",
        "        'RMSE': RMSE_r,\n",
        "        'MAPE': MAPE_r,\n",
        "        'R2': r2_r,\n",
        "        'Accuracy': accuracy_r\n",
        "       }\n",
        "metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "metric_df"
      ],
      "metadata": {
        "id": "VT0vD8MOd1VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We have chosen XGboost as our final prediction model with hyperparameters {'learning_rate': 0.1, 'max_depth': 13} as it is very clear from above dataframe that it has given the highest accuracy (97%), least MAPE (3%) and maximum r2 score(0.94) on the testing dataset among all other models."
      ],
      "metadata": {
        "id": "ZDVC6zl6eJOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "XGBoost (eXtreme Gradient Boosting) provides an efficient implementation of the gradient boosting framework. It is designed for both linear and tree-based models, and it is useful for large datasets. The basic idea behind XGBoost is to train a sequence of simple models, such as decision trees, and combine their predictions to create a more powerful model. Each tree is trained to correct the errors made by the previous trees in the sequence and this known as boosting.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing pickle module\n",
        "import pickle"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "filename='df_regression.pkl'\n",
        "\n",
        "# serialize process (wb=write byte)\n",
        "pickle.dump(xgb_Rand_R,open(filename,'wb'))"
      ],
      "metadata": {
        "id": "3qsI5y4MmhUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# unserialize process (rb=read byte)\n",
        "Regression_model= pickle.load(open(filename,'rb'))\n",
        "\n",
        "# Predicting the unseen data(test set)\n",
        "Regression_model.predict(X_test)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if we are getting the same predicted values\n",
        "y_test_rand_xgbR_pred"
      ],
      "metadata": {
        "id": "GJeX1e-Amugw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here.\n",
        "\n",
        "Final Conclusion:\n",
        "\n",
        "1.In holiday or non-working days there is demands in rented bikes.\n",
        "\n",
        "2.There is a surge of high demand in the morning 8AM and in evening 6PM as the people might be going to their work at morning 8AM and returing from their work at the evening 6PM.\n",
        "\n",
        "3.People prefered more rented bikes in the morning than the evening.\n",
        "\n",
        "4.When the rainfall was less, people have booked more bikes except some few cases.\n",
        "\n",
        "7.The Temperature, Hour & Humidity are the most important features that positively drive the total rented bikes count.\n",
        "\n",
        "8.After performing the various models the lightGBM and Catboost found to be the best model that can be used for the Bike Sharing Demand Prediction since the performance metrics (mse,rmse) shows lower and (r2,adjusted_r2) shows a higher value for the Random Forest and XG Boost models !\n",
        "\n",
        "9.We can use either Random forest or XG Boost model for the bike rental stations."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}